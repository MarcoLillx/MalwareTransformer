# MalwareTransformers

> MALWARE DETECTION TRAMITE ANALISI DI API CALLS E LARGE LANGUAGE MODELS

<p align="center">
 <img src="https://cdn.pixabay.com/photo/2022/05/30/11/25/cyber-security-7231027_1280.jpg"/>
</p>

## Istruzioni per l'installazione

L'intero progetto può essere eseguito su **Google Colab** (utilizzando la T4 GPU Free o una a pagamento) oppure su **Jupyter** (se si dispone di abbastanza memoria).

Se viene utilizzato Jupyter è consigliato creare un ambiente virtuale su **Anaconda** in cui installare le dipendenze:
- `conda create --name malware-transformer` (per creare l'ambiente virtuale)
- `conda activate malware-transformer` (per attivare l'ambiente virtuale)


Le librerie e dipendenze utilizzate sono presenti all'interno del file requirements.txt. Per installarle bisonga posizionarsi all'interno della directory principale e digitare:

	`pip install -r requirements.txt`

> NOTA: Se si riscontrano problemi durante l'installazione delle librerie tramite linea di comando, è possibile installarle automaticamente all'interno del notebook selezionato eseguendo la prima cella relativa alle dipendenze (presente in ogni notebook).

Clicca [qui](https://huggingface.co/settings/tokens) per creare un HuggingFace Token in modalità read, necessario per poter utilizzare i modelli Llama2 e Llama3.
> NOTA: il token è univoco e sarà visibile soltanto una volta dopo la creazione, è consigliabile quindi salvarlo.

## Istruzioni per l'esperimento di Sequence Classification

Il primo esperimento si trova in *"MalwareTransformers/Fine-tuning/Sequence Classification"*.

È presente un'ulteriore cartella *small_experiments* in cui è stato eseguito l'esperimento di sequence classification utilizzando 400 tuple (invece di 1000) selezionate casualmente dal dataset *drebin_new.csv*. 

Se si utilizza **Google Colab** è necessario selezionare inizialmente la GPU che si intende utilizzare ed in seguito:
1. Caricare localmente il notebook *LLAMA_3_Fine_Tuning_for_Sequence_Classification_1000samples.ipynb* per Llama3 o *LLAMA_2_Fine_Tuning_for_Sequence_Classification_1000samples.ipynb* per Llama2.
2. Eseguire la prima cella, relativa all'installazione delle dipendenze.
3. Effettuare il login su HuggingFace-Hub inserendo il proprio token.
4. Caricare localmente  il dataset che si trova in **"MalwareTransformers\Datasets\drebin dataset\drebin_new.csv"** e seguire le istruzioni all'interno della cella relativa al caricamento del dataset.
5. Eseguire le restanti celle.
6. Collegare l'account Drive per salvare i parametri di configurazione e il modello addestrato, se desiderato.

Se si utilizza **Jupyter Notebook** è necessario:.
1. Avviare il notebook relativo a Llama2 o Llama3.
2. Nel caso in cui il processo iniziale di [installazione](#istruzioni-per-l'installazione) delle dipendenze non fosse andato a buon fine, eseguire la prima cella.
3. Effettuare il login su HuggingFace-Hub da terminale inserendo il proprio token:
		`huggingface-cli login`
4. Eseguire le restanti celle.
5. Collegare l'account Drive per salvare i parametri di configurazione e il modello addestrato, se desiderato.


## Istruzioni per l'esperimento di Text Generation
Il secondo esperimento si trova in *"MalwareTransformers\Fine-tuning\Text Generation"*:
### <u>LLAMA2</u>

Se si utilizza **Google Colab**:

1. Caricare localmente FineTuningLlama2.ipynb.
2. Eseguire la prima cella, relativa all'installazione delle dipendenze.
3. Effettuare il login su HuggingFace-Hub inserendo il proprio token.
4. Caricare localmente i datasets che si trovano in **"MalwareTransformers\Datasets\drebin dataset"**:
> i file sono train_data.jsonl e test_data.jsonl
5. Collegare Weight and Biases utilizzando la propria [chiave API](https://wandb.ai/settings#api).
6. Eseguire le restanti celle fino alla fase di training (Sezione 5).
7. Riavviare la sessione e testare, quindi, il modello addestrato(Sezione 6).

Se si utilizza **Jupyter Notebook**:
1. Nel caso in cui il processo iniziale di [installazione](#istruzioni-per-l'installazione) delle dipendenze non fosse andato a buon fine, eseguire la prima cella.
2. Effettuare il login su HuggingFace-Hub da terminale inserendo il proprio token:
		`huggingface-cli login`
3. Collegare Weight and Biases utilizzando la propria [chiave API](https://wandb.ai/settings#api).
4. Eseguire le restanti celle fino alla fase di training (Sezione 5).
5. Riavviare il Kernel (*Kernel > Restart Kernel*) e testare, quindi, il modello addestrato(Sezione 6).

### <u>LLAMA3</u>
All'interno della cartella small-experiments è presente un notebook in cui è stato testato il modello Llama 3 utilizzando solamente 200 tuple selezionate casualmente dal dataset. 

Se si utilizza **Google Colab**:

1. Caricare localmente FineTuningLlama3.ipynb.
2. Eseguire la prima cella, relativa all'installazione di Unsloth.
3. Caricare localmente i datasets che si trovano in **"MalwareTransformers\Datasets\drebin dataset\small-version"**:
> i file sono train_data.jsonl e test_data.jsonl
4. Eseguire le restanti celle e testare il modello addestrato.

Se si utilizza **Jupyter Notebook**:
1. Aprire FineTuningLlama3.ipynb
2. Eseguire la prima cella, relativa all'installazione di Unsloth.
3. Eseguire le restanti celle e testare il modello addestrato.

## Istruzioni per l'esperimento di Prompt Engineering
### <u>LLAMA2</u>
Quest'ultimo esperimento va eseguito su Jupyter Notebook e si trova in *"MalwareTransformers\Prompt-engineering\llamacpp.ipynb"*.

In *promptengineering.ipynb* è stato seguito un approccio differente(più lento).

Le istruzioni sono le seguenti:
1. Installare llama.cpp: 

`pip install llama-cpp-python`
oppure `pip install llama-cpp-python==0.1.48`

2. Scaricare da [qui](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q2_K.gguf) il modello in formato GGUF e salvarlo all'interno della cartella model ("*MalwareTransformers\Prompt-engineering\model*")
3. Aprire il file llamacpp.ipynb
4. Eseguire le celle e testare il modello

### <u>LLAMA3</u>

Se non si vuole utilizzare l'app LM Studio, è possibile testare il modello Llama3 sul notebook <u>***llamacpp.ipynb***</u> cambiando


`my_model_path = "./model/llama-2-7b-chat.Q2_K.gguf"` 

in

`my_model_path = "./model/Meta-Llama-3-8B-Instruct.Q2_K.gguf"`


ed eseguire quindi le celle presenti all'interno del notebook.

Per utilizzare, invece, l'app LM Studio seguire i seguenti passi:

1. Scaricare l'app [LM Studio](https://lmstudio.ai/)
2. Posizionarsi all'interno del directory di installazione dell'app ed aprire la cartella "*lmstudio-community*"
3. Creare una cartella "*Meta-Llama3-8B*"
4. Scaricare da [qui](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/blob/main/Meta-Llama-3-8B-Instruct.Q2_K.gguf)  il modello in formato GGUF e salvarlo all'interno di "*Meta-Llama3-8B*" 
5. Selezionare sul menu a sinistra l'icona della chat
6. Premere "Select a model to load" e cliccare sul modello che uscirà in basso
7. Avviare la chat e testare il modello